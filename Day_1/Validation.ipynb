{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/icomse/5th_workshop_MachineLearning.git\n",
    "import os\n",
    "os.chdir('5th_workshop_MachineLearning/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cld=pd.read_csv('HCEPDB_100K_cleaned.csv') # change where it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(cld['pce'],kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cld = cld[cld['pce']!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(cld['pce'],kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a number of different ways to standardize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a range of different ways to impute data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We always want to minimize the MSE of the TEST dataset, not the error of the TRAINING dataset. \n",
    "\n",
    "When **training** a machine learning model, we split our dataset into a **training dataset** and a **test dataset**. The training dataset is used to train our machine learning model. The test dataset is used to verify the accuracy of our trained model. It is **very** important that the test dataset and training dataset are completely separate, otherwise you could overestimate the accuracy of your model.\n",
    "\n",
    "We rarely have an indepedent test dataset. In practice, we will randomly split our sample dataset into a training dataset and test dataset. This process is called **validation**. Validation is typically used in two scenarios: **model selection** (choosing the proper level of flexibility) and **model assessment** (evaluating a model's performance). We will take a look at both use cases.\n",
    "<img width=\"304\" height=\"175\" alt=\"Image result for training test dataset\" src=\"http://scott.fortmann-roe.com/docs/docs/MeasuringError/holdout.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scikit-learn` handily has a function that does this split for us. But first, let's create a datset that we can practice on using a regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 10, 40)\n",
    "# create some polynomial data with noise.\n",
    "np.random.seed(80309)\n",
    "noise_scale = 0.25\n",
    "y = 0.2*x + 1*x**2 - 0.3*x**3 + 0.021*x**4 - 0.00018*x**5 +  noise_scale*np.random.rand(len(x))*(x+3)\n",
    "plt.scatter(x, y, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create some polynomial features we can fit to\n",
    "X = pd.DataFrame()\n",
    "ndegree = 20\n",
    "for d in range(ndegree):\n",
    "    X[f'x{d}'] = x**d "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use validation to select the degree of flexibility of our linear model. First, the split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression().fit(X_train[['x1']], y_train)\n",
    "y_pred_test = reg.predict(X_test[['x1']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit to just a simple line . . . not so great."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y, alpha=0.3)\n",
    "plt.plot(X_test[['x1']], y_pred_test, linewidth=2,color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get a measure of the degree of quality of fit using the mean squared error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's repeat the linear regression while increasing the degree of our polynomial, and plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "ax.scatter(x, y, alpha=0.3)\n",
    "npoly = len(X.columns)+1\n",
    "mse_test = np.zeros(npoly)\n",
    "mse_train = np.zeros(npoly)\n",
    "\n",
    "# DISCUSS WHAT EACH LINE \n",
    "for i in range(1, npoly):\n",
    "    reg = LinearRegression().fit(X_train[X_train.columns[:i]], y_train)    \n",
    "    y_pred_train = reg.predict(X_train[X_test.columns[:i]])\n",
    "    y_pred_test = reg.predict(X_test[X_test.columns[:i]])\n",
    "    ax.scatter(X_test[['x1']], y_pred_test, marker='.',label=\"degree poly = \"+str(i),s=100,alpha=0.5)    \n",
    "    mse_test[i-1] = mean_squared_error(y_test, y_pred_test)\n",
    "    mse_train[i-1] = mean_squared_error(y_train, y_pred_train)\n",
    "ax.legend(loc=(1.1,0))\n",
    "plt.ylim([-10,10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "ax.plot(np.linspace(1, npoly-1,npoly-1), mse_train[:-1], linewidth=4,alpha=0.5,color='orange',label='train')\n",
    "ax.plot(np.linspace(1, npoly-1,npoly-1), mse_test[:-1], linewidth=4, alpha=0.5,color='blue',label='test')\n",
    "ax.set_ylabel('Mean Squared Error')\n",
    "ax.set_xlabel('Degree of Polynomial')\n",
    "ax.set_ylim([0,15])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the difference is between the MSE of the test set and the training set?\n",
    "Where would this data suggest we use as a degree of fit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This validation method has two disadvantages:\n",
    "    \n",
    "* The test error is variable, depending on your training dataset. If we used a different training dataset, we'd have a different error\n",
    "* We only use a subset of the training dataset to create our model. The fewer the observations, the worse the final model (we actually overestimate the test error rate!) What happens if we run a few times?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "ntrials = 40\n",
    "for run in range(4):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    mse = np.zeros(npoly)\n",
    "\n",
    "    for i in range(1, npoly):\n",
    "        reg = LinearRegression().fit(X_train[X_train.columns[0:i]], y_train)\n",
    "        y_pred = reg.predict(X_test[X_test.columns[0:i]])\n",
    "\n",
    "        mse[i-1] = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    \n",
    "    ax.plot(np.linspace(1, npoly-1,npoly-1), mse[:-1], linewidth=4, alpha=0.1, color ='b')\n",
    "    ax.set_ylabel('Mean Squared Error')\n",
    "    ax.set_xlabel('Degree of Polynomial')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One common alternative to is **K-fold cross-validation**. In this approach, we split the original dataset into *k* units. In on training-test configuration, *k-1* of the units will be used as a training dataset, while the remaining unit will be used as a test dataset. We train a model *k* times, switching out the test dataset each time, and use the average error as an estimate of the actual test error.\n",
    "\n",
    "<img width=\"608\" height=\"334\" alt=\"Image result for k-fold cross-validation\" src=\"https://cdn-images-1.medium.com/max/1600/1*me-aJdjnt3ivwAurYkB7PA.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hacking Time!\n",
    "\n",
    "1. In the below code, add a comment on what each line that uses the `Kfold` module does. . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfold = 10\n",
    "kf = KFold(n_splits=nfold,shuffle=True) \n",
    "\n",
    "# we need shuffle=True, since the data is ordered\n",
    "print('number of splits =', kf.get_n_splits(X, y)) \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "mse = np.zeros((npoly, nfold))\n",
    "j = 0\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    # what are train_index and test_index? You can comment out this line\n",
    "    # below once you figure out\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    for i in range(1, npoly+1):\n",
    "        reg = LinearRegression().fit(X_train[X_train.columns[:i]], y_train)\n",
    "        y_pred = reg.predict(X_test[X_test.columns[:i]])\n",
    "        mse[i-1, j] = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    ax.plot(np.linspace(1, npoly-1, npoly), mse[:, j], linewidth=4, color='b', alpha=0.09)\n",
    "    ax.set_ylabel('Mean Squared Error')\n",
    "    ax.set_xlabel('Degree of Polynomial')\n",
    "    j += 1\n",
    "    \n",
    "avg_mse = mse.mean(axis=1)\n",
    "ax.plot(np.linspace(1, npoly-1, npoly), avg_mse, color='purple',label='Kfold')\n",
    "ax.set_ylim([0,3])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "2. Now, trying perform hyperparameter tuning; running this process of looking at the the test MSE through with different numbers of 'folds', different numbers of data points, and with different amounts of noise (the variable `noise_scale` above. Try to break things!\n",
    "\n",
    "Report what you find to be the optimal degree polynomial in the Day 1 collaborative notes as a function of the noise, and as a function of number of data points include "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

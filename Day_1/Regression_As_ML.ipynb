{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKt0TtydiYES"
      },
      "source": [
        "### Regression as machine learing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hliZw7DiYET"
      },
      "outputs": [],
      "source": [
        "!pip install ase\n",
        "!git clone https://github.com/icomse/5th_workshop_MachineLearning.git\n",
        "import os\n",
        "os.chdir('5th_workshop_MachineLearning/data')\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qo6wZ3mKiYET"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "# the names in sklearn tend to be pretty long, so we will import the individual\n",
        "# objects rather than the whole thing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JjhvnBUiYET"
      },
      "source": [
        "### Let's read in some data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdVTQYMCiYET"
      },
      "source": [
        "This is some artificial data we will be playing around with to illustrate some important concepts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbEebCVAiYEU"
      },
      "outputs": [],
      "source": [
        "dlin = pd.read_csv('linmod.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "I7g3dlbSiYEU"
      },
      "source": [
        "What does this data look like? Inspect directly and plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbTFtN7tiYEU"
      },
      "outputs": [],
      "source": [
        "dlin.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Av8e0mL3iYEU"
      },
      "outputs": [],
      "source": [
        "dlin.plot(x='inputs',y='outputs',kind='scatter')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgoFn4YwiYEW"
      },
      "source": [
        "Now, let's train a linear model with the data! First, create the model. It starts out as just an empty object, that we need to fill."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9EU74C_iYEW"
      },
      "outputs": [],
      "source": [
        "linmodel = LinearRegression()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KSSVGrziYEW"
      },
      "source": [
        "Now look up the documentation as to what to do to fit the model. When you fit it, what do you get back? NOTE: `LinearRegression` expects that that the input X is a 2D arrray, of size `[n_features, n_samples]`, because it is designed to handle the general case that you are doing multiple variable linear regression. So if you pass in a 1D array, it will error. Read the error message you get carefully for a quick fix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJQOBnruiYEW"
      },
      "outputs": [],
      "source": [
        "X = dlin['inputs'].values.reshape(-1, 1)\n",
        "Y = dlin['outputs']\n",
        "linmodel.fit(X,Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xgDVumziYEW"
      },
      "source": [
        "OK, you have a model. We next want to plot the prediction of the model and the points on the same plot?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZakOg-IiYEW"
      },
      "outputs": [],
      "source": [
        "xvals = np.linspace(10,70,100)\n",
        "ypred = linmodel.predict(xvals.reshape(-1,1))  # use this to apply the model\n",
        "plt.plot(xvals,ypred,c='k')\n",
        "plt.scatter(X,Y)\n",
        "plt.xlabel('inputs')\n",
        "plt.ylabel('outputs')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLXTZejHiYEW"
      },
      "source": [
        "What methods does the `LinearRegression` have? What information can you get from it?  What are the parameters of a 1D linear regression?  What is a measure of how good the model is?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQl4JZ7qiYEW"
      },
      "outputs": [],
      "source": [
        "linmodel?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWG43uLHiYEW"
      },
      "outputs": [],
      "source": [
        "print(linmodel.intercept_)\n",
        "print(linmodel.coef_)\n",
        "print(linmodel.score(X,Y))  # in this case, the score gives the R^2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "my6vHy9fiYEW"
      },
      "source": [
        "Let's play around with (over)fitting!  There's a little bit of curvature - in the data; maybe we should try a polynomial fit?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mA1feUQLiYEW"
      },
      "source": [
        "Let's actually just use a sample of the data set to train; we want to go up to the number of data points, and polynomial regression is numerically unstable with large numbers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tdrs09BaiYEW"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhC3MEQKiYEX"
      },
      "source": [
        "We make a polynomial fit using sklearn by taking powers of the input features.  `scikit-learn` has a function for this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKEeXHsTiYEX"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0upa2foiYEX"
      },
      "source": [
        "To use it, you create a `PolynomialFeatures` object of the degree you are interested in, then take the powers of the data using the fit_transform method of `PolynomialFeatures`. See example (you will have to supply your own X, which is the same input as `LinearRegression.fit` takes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZNfXG3uiYEX"
      },
      "outputs": [],
      "source": [
        "pf = PolynomialFeatures(degree=2)\n",
        "pX_train = pf.fit_transform(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUNye0i6iYEX"
      },
      "outputs": [],
      "source": [
        "print(X_train[:5])\n",
        "print(pX_train[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hb20fbP7iYEX"
      },
      "source": [
        "it's a degree $\\times$ n_data array; with each column the input data to power $n$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrqIDdt3iYEY"
      },
      "source": [
        "Now, lets see how the model fits the data.  Inspect the model parameters and the $R^2$, and plot the fit and the curve."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPSW_9lCiYEY"
      },
      "outputs": [],
      "source": [
        "linmodel.fit(pX_train,Y_train)\n",
        "print(linmodel.intercept_)\n",
        "print(linmodel.coef_)\n",
        "print('R^2 (train) =', linmodel.score(pX_train,Y_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RcG01fIyiYEY"
      },
      "outputs": [],
      "source": [
        "xvals = np.linspace(10,70,100)\n",
        "px = pf.fit_transform(xvals.reshape(-1,1))\n",
        "ypredict = linmodel.predict(px)\n",
        "plt.plot(xvals,ypredict,c='k')\n",
        "plt.scatter(X_train,Y_train)\n",
        "plt.xlabel('inputs')\n",
        "plt.ylabel('outputs')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otZ9-rXqiYEY"
      },
      "source": [
        "Now test the model on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DmHy6eMNiYEY"
      },
      "outputs": [],
      "source": [
        "pX = pf.fit_transform(X_test)\n",
        "plt.plot(xvals,ypredict,c='k')\n",
        "plt.scatter(X_test,Y_test)\n",
        "plt.xlabel('inputs')\n",
        "plt.ylabel('outputs')\n",
        "print('R^2 =', linmodel.score(pX,Y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nM47GscIiYEY"
      },
      "source": [
        "Remember, it's the TESTING MSE (or $R^2$) that we want to minimize, not the training MSE.  If the training MSE is low, but the testing is high, then the model is overfit.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKLzIFzkiYEY"
      },
      "source": [
        "**HACKING TIME**: Now, go back and try a higher polynomial degree. Note that polynomial fitting is not very numerically stable, so if you fit to too many points, then you start to have problems.  What problems do you have?\n",
        "\n",
        "Can you plot $R^2$ (i.e. linmodel.score) for the test and training set versus polynomial degree?  What do you see?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-fQFrwbiYEY"
      },
      "outputs": [],
      "source": [
        "pf = PolynomialFeatures(degree=4)\n",
        "pX_train = pf.fit_transform(X_train) # transform the features into a x^0 . . . x^15\n",
        "linmodel.fit(pX_train,Y_train) # train the model on the data\n",
        "print('R^2 (train) =', linmodel.score(pX_train,Y_train))\n",
        "pxvals = pf.fit_transform(xvals.reshape(-1,1)) # generate the polynomial input data\n",
        "ypredict = linmodel.predict(pxvals) # predict the curve of the model.\n",
        "plt.plot(xvals,ypredict,c='k')\n",
        "plt.scatter(X_test,Y_test,label='test',alpha=0.5)\n",
        "plt.scatter(X_train,Y_train,label='train',alpha=0.5)\n",
        "plt.legend()\n",
        "pX_test = pf.fit_transform(X_test)  # what is the transformed test data, so we can compute the score\n",
        "print('R^2 (test) =', linmodel.score(pX_test,Y_test)) # how well does the test data fit?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yl8zZQKhiYEZ"
      },
      "outputs": [],
      "source": [
        "R2test = list()\n",
        "R2train = list()\n",
        "ps = list()\n",
        "for p in range(2,20):\n",
        "    pf = PolynomialFeatures(degree=p)\n",
        "    pX_train = pf.fit_transform(X_train)\n",
        "    linmodel.fit(pX_train,Y_train)\n",
        "    R2train.append(linmodel.score(pX_train,Y_train))\n",
        "    pxvals = pf.fit_transform(xvals.reshape(-1,1)) # generate the polynomial input data\n",
        "    ypredict = linmodel.predict(pxvals)\n",
        "    pX_test = pf.fit_transform(X_test)\n",
        "    R2test.append(linmodel.score(pX_test,Y_test))\n",
        "    ps.append(p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGfhYAcMiYEZ"
      },
      "outputs": [],
      "source": [
        "plt.plot(ps,R2test,label='test set')\n",
        "plt.plot(ps,R2train,label='training set')\n",
        "plt.xlabel('polynomial degree')\n",
        "plt.ylabel('$R^2$')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_m7meWA7iYEZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgWKezSwiYEZ"
      },
      "source": [
        "Generally the training set will get a bit better as the model gets more complex, whereas the test model gets worse as the trained model starts to overfit."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
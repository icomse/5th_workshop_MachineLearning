{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, make sure the notebook is aware of the workshop data sets\n",
    "!pip install ase\n",
    "!git clone https://github.com/icomse/5th_workshop_MachineLearning.git\n",
    "import os\n",
    "os.chdir('5th_workshop_MachineLearning/data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression as machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the simplest models we can! Plain old fitting to a line; the simplest model there can be. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the names in sklearn tend to be pretty long, so we will import the individual\n",
    "# objects rather than the whole library \n",
    "\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's read in some data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is some artificial data we will be playing around with to illustrate some important concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlin = pd.read_csv('linmod.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "What does this data look like? Inspect directly and plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlin.plot(x='inputs',y='outputs',kind='scatter')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train a linear model with the data! First, create the model. It starts out as just an empty object, that we need to fill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linmodel = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now look up the documentation as to what to do to fit the model. When you fit it, what do you get back? NOTE: `LinearRegression` expects that that the input X is a 2D arrray, of size `[n_features, n_samples]`, because it is designed to handle the general case that you are doing multiple variable linear regression. So if you pass in a 1D array, it will error. Read the error message you get carefully for a quick fix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dlin['inputs'].values.reshape(-1, 1)\n",
    "Y = dlin['outputs']\n",
    "linmodel.fit(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, you have a model. We next want to plot the prediction of the model and the points on the same plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvals = np.linspace(10,70,100)\n",
    "ypred = linmodel.predict(xvals.reshape(-1,1))  # use this to apply the model\n",
    "plt.plot(xvals,ypred,c='k')\n",
    "plt.scatter(X,Y)\n",
    "plt.xlabel('inputs')\n",
    "plt.ylabel('outputs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What methods does the `LinearRegression` have? What information can you get from it?  What are the parameters of a 1D linear regression?  What is a measure of how good the model is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linmodel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(linmodel.intercept_)\n",
    "print(linmodel.coef_)\n",
    "print(linmodel.score(X,Y))  # in this case, the score gives the R^2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's play around with (over)fitting!  There's a little bit of curvature - in the data; maybe we should try a polynomial fit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's actually just use a sample of the data set to train; we want to go up to the number of data points, and polynomial regression is numerically unstable with large numbers.  There are tools doing this type of split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make a polynomial fit using sklearn by taking powers of the input features.  `scikit-learn` has a function for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use it, you create a `PolynomialFeatures` object of the degree you are interested in, then take the powers of the data using the fit_transform method of `PolynomialFeatures`. See example (you will have to supply your own X, which is the same input as `LinearRegression.fit` takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = PolynomialFeatures(degree=2)\n",
    "pX_train = pf.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[:5])\n",
    "print(pX_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it's a degree $\\times$ n_data array; with each column the input data to power $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets see how the model fits the data.  Inspect the model parameters and the $R^2$, and plot the fit and the curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linmodel.fit(pX_train,Y_train)\n",
    "print(linmodel.intercept_)\n",
    "print(linmodel.coef_)\n",
    "print('R^2 (train) =', linmodel.score(pX_train,Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvals = np.linspace(10,70,100)\n",
    "px = pf.fit_transform(xvals.reshape(-1,1))\n",
    "ypredict = linmodel.predict(px)\n",
    "plt.plot(xvals,ypredict,c='k')\n",
    "plt.scatter(X_train,Y_train)\n",
    "plt.xlabel('inputs')\n",
    "plt.ylabel('outputs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pX = pf.fit_transform(X_test)\n",
    "plt.plot(xvals,ypredict,c='k')\n",
    "plt.scatter(X_test,Y_test)\n",
    "plt.xlabel('inputs')\n",
    "plt.ylabel('outputs')\n",
    "print('R^2 =', linmodel.score(pX,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, it's the TESTING MSE (or $R^2$) that we want to minimize, not the training MSE.  If the training MSE is low, but the testing is high, then the model is overfit.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HACKING TIME**: Now, go back and try a higher polynomial degree. Note that polynomial fitting is not very numerically stable, so if you fit to too many points, then you start to have problems.  What problems do you have?\n",
    "\n",
    "Can you plot $R^2$ (i.e. linmodel.score) for the test and training set versus polynomial degree?  What do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally the training set will get a bit better as the model gets more complex, whereas the test model gets worse as the trained model starts to overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilinear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's well and good, but that is just one input.  Usually, we have LOTS of features and want to use all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cld=pd.read_csv('HCEPDB_100K_cleaned.csv') # change where it is\n",
    "# (but first you might refresh yourself on what is contained\n",
    "# we'll use a smaller sample to make it go faster, and to make the problem a bit harder.\n",
    "cld = cld[cld['pce']!=0] # clean out the data where PCE = 0 (not obtained)\n",
    "cld_sample = cld.sample(n=1000,random_state=12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic principle (and code!) is the same as ordinary least squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['mass', 'voc', 'jsc', 'e_homo_alpha', 'e_gap_alpha', 'e_lumo_alpha']\n",
    "observable = 'pce'\n",
    "X = cld_sample[features]\n",
    "Y = cld_sample[observable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now fit the model\n",
    "linmodel = LinearRegression() \n",
    "linmodel.fit(X,Y)\n",
    "linmodel.score(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, there are more coefficients! One for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linmodel.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One think to take into account, though. What if the data has different units, or is different types?  How will the coefficients change? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now make a test-train split. We'll reserve most of the data for testing (we want to make it harder to fit!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(cld_sample, test_size=0.95, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use data standardization or normalization to remove this effect.  Now, it's not always the best thing to do.  If you have two variables that are the same units, it could be that the distances DO matter.  \n",
    "\n",
    "Minimum, it makes sure that the model does not depend on the units used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(cld_sample[features])\n",
    "train_standardized = pd.DataFrame(data=scaler.transform(train[features]),columns=features)\n",
    "test_standardized = pd.DataFrame(data=scaler.transform(test[features]),columns=features)\n",
    "# stick back in the unscaled observables\n",
    "test_standardized['pce']=test['pce'].values \n",
    "train_standardized['pce']=train['pce'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For lasso and ridge regression, $R^2$ doesn't quite make sense, because the extra terms messes with calculation of the total sum of squares.  Instead, let's just look at the means squared error. For simple linear regression (no matter how many variables.  Also, the coefficients of simple linear regression can take into account the change of units (it's the shrinkage term that causes problems), but we'll do everything with standardized data to make it simpler. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Just getting it set up with more rigor: Multiple linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train linear model \n",
    "MLR=LinearRegression()\n",
    "MLR.fit(train[features],train[[observable]])\n",
    "\n",
    "# make predictions on test and train set \n",
    "trainpred=MLR.predict(train[features])\n",
    "testpred=MLR.predict(test[features])\n",
    "\n",
    "#make parity plot \n",
    "plt.figure(figsize=(7,7))\n",
    "plt.scatter(test[observable],testpred,color='r', label='Test')\n",
    "plt.scatter(train[observable],trainpred, label='Training')\n",
    "plt.plot(lw=4,color='black')\n",
    "plt.legend()\n",
    "plt.xlabel('Actual Output')\n",
    "plt.ylabel('Predicted Output')\n",
    "\n",
    "#calculate the test and train error\n",
    "print(\"Train error\",mean_squared_error(train[observable],trainpred))\n",
    "print(\"Test error\",mean_squared_error(test[observable],testpred))\n",
    "print(MLR.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Ridge Regression (same data as Part 1)\n",
    "\n",
    "* The ridge coefficients minimize $RSS + \\lambda \\sum_{j=1}^{p}\\beta_j^2$\n",
    "* There is an additional **penalty** in error for having large coefficients!\n",
    "* Note: ISLR shows the tuning parameter as $\\lambda$, but it is $\\alpha$ in `sk-learn`\n",
    "* Goal here: train models as a function of the regularization parameter \n",
    "* The X's **must** be be standardized for ridge regression to work correctly \n",
    "* Some methods in `sk_learn` also do automatic selection of shrinkage coefficient - you can try those out afterwards. \n",
    "* **For the next section, I suggest on your own you test out what normalization in ridge does, by executing the following code first WITHOUT and then WITH normalization** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of a single instance of ridge regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge=Ridge()\n",
    "a=2.0\n",
    "ridge.set_params(alpha=a)\n",
    "ridge.fit(train_standardized[features],train_standardized[observable])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predict = ridge.predict(train_standardized[features])\n",
    "test_predict = ridge.predict(test_standardized[features])\n",
    "print(mean_squared_error(train_standardized[observable],train_predict))\n",
    "print(mean_squared_error(test_standardized[observable],test_predict))\n",
    "print(ridge.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge=Ridge()\n",
    "a=8.0\n",
    "ridge.set_params(alpha=a)\n",
    "ridge.fit(train_standardized[features],train_standardized[observable])\n",
    "train_predict = ridge.predict(train_standardized[features])\n",
    "test_predict = ridge.predict(test_standardized[features])\n",
    "print(mean_squared_error(train_standardized[observable],train_predict))\n",
    "print(mean_squared_error(test_standardized[observable],test_predict))\n",
    "print(ridge.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "$\\alpha$ (or $\\lambda$) is an example of a *hyperparameter*.  It's a variable that will change exactly what the model is doing.  The parameters of the models are the coefficients of the ridge regression; each choice of parameter gives a different model, but each choice of hyperparameters leads to a different model. Deep learning methods often have *many* hyperparameters. Tuning hyperparameters is a key part of building machine learning models.  So let's tune a hyperparameter!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hacking**: \n",
    "1. Redo the linear regression with standardized data.  How do the coefficents change when you add regression?\n",
    "2. Vary $\\alpha$ and see how the predictions and coefficients change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "plt.scatter(train_standardized[observable],train_predict, label='Training')\n",
    "plt.scatter(test_standardized[observable],test_predict,color='r', label='Test')\n",
    "plt.plot([-2.5,10],[-2.5,10],lw=4,color='black')\n",
    "plt.legend()\n",
    "plt.xlabel('Actual Output')\n",
    "plt.ylabel('Predicted Output')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of searching the $\\alpha$ ($\\lambda$ in the notes) space in Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RR vs lambda (based on sklearn tutorial)\n",
    "coefs = []\n",
    "trainerror = []\n",
    "testerror = []\n",
    "\n",
    "model = Ridge()\n",
    "\n",
    "lambdas = np.logspace(-4,5,100)\n",
    "# loop over lambda values (strength of regularization and find the errors)\n",
    "for l in lambdas:\n",
    "    model.set_params(alpha=l)\n",
    "    model.fit(train_standardized[features],train_standardized[observable])\n",
    "    coefs.append(model.coef_)\n",
    "    trainerror.append(mean_squared_error(train_standardized[observable],model.predict(\n",
    "        train_standardized[features])))\n",
    "    testerror.append(mean_squared_error(test_standardized[observable],model.predict(\n",
    "        test_standardized[features])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is being plotted here? \n",
    "plt.figure(figsize=(10,3))\n",
    "plt.subplot(121)\n",
    "plt.plot(lambdas,coefs)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('$\\lambda$')\n",
    "plt.ylabel('coefs')\n",
    "plt.title('RR coefs vs $\\lambda$')\n",
    "plt.subplot(122)\n",
    "plt.plot(lambdas,trainerror,label='train error')\n",
    "plt.plot(lambdas,testerror,label='test error')\n",
    "plt.xscale('log')\n",
    "plt.ylim([0.01,1.5])\n",
    "plt.xlabel('$\\lambda$')\n",
    "plt.ylabel('error')\n",
    "plt.legend(loc=1)\n",
    "plt.title('error vs $\\lambda$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pause and discuss with a partner**: What is happening in these plots? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is best (least overfit, best test MSE) at between $\\lambda$ ($\\alpha$) = $10^1$ and $10^2$, specifically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas[np.argmin(testerror)] #the lambda where the MSE is minimized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: LASSO regression  (same data as Part 1)\n",
    "\n",
    "* The LASSO improves over ridge regression by also providing a variable selection tool!\n",
    "* The LASSO minimizer is $RSS + \\lambda \\sum_{j=1}^{p}\\lvert\\beta_j\\rvert$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hacking time**: Plot the LASSO regression coefficients versus the magnitude of the shrinkage term, in the same way the ridge regression was done above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/4v/g0zjp6c95535931grpmdq5840000gp/T/ipykernel_40963/3925523437.py\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrainerror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtesterror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mlambdas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLasso\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# we already standardized the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# also based on sklearn tutorials\n",
    "# think about what is happening in this cell?\n",
    "coefs = []\n",
    "trainerror = []\n",
    "testerror = []\n",
    "lambdas = np.logspace(-5,3,100)\n",
    "model = Lasso() # we already standardized the data\n",
    "\n",
    "# loop over lambda values (strength of regularization)\n",
    "for l in lambdas:\n",
    "    model.set_params(alpha=l,max_iter=100000)\n",
    "    model.fit(train_standardized[features],train_standardized[observable])\n",
    "    coefs.append(model.coef_)\n",
    "    trainerror.append(mean_squared_error(train_standardized[observable],model.predict(\n",
    "        train_standardized[features])))\n",
    "    testerror.append(mean_squared_error(test_standardized[observable],model.predict(\n",
    "        test_standardized[features])))\n",
    "\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.subplot(121)\n",
    "plt.plot(lambdas,coefs)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('$\\lambda$')\n",
    "plt.ylabel('coefs')\n",
    "plt.title('Lasso coefs vs $\\lambda$')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(lambdas,trainerror,label='train error')\n",
    "plt.plot(lambdas,testerror,label='test error')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('$\\lambda$')\n",
    "plt.ylabel('error')\n",
    "plt.xlim(1e-4,1e0)\n",
    "plt.ylim(0.2,2.0)\n",
    "plt.legend()\n",
    "plt.title('error vs $\\lambda$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_lambda = lambdas[np.argmin(testerror)]  # at labmbda =0.28 the test error is minimized.\n",
    "print(min_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs[np.argmin(testerror)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs[np.argmin(testerror)]==0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the coefficients are zero. Specifically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(features)[coefs[np.argmin(testerror)]==0.0]  # need to convert to numpy array to use masking. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So LASSO suggest that the best model would have just:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(features)[coefs[np.argmin(testerror)]!=0.0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### General linear models \n",
    "\n",
    "In the last class, we generated model based on the hypothesis that the $PCE$ of a candidate organic photovoltaic can be modeled as a contribution of the molecule's $mass$, $VOC$ and $E_{LUMO}$ values:  $PCE = \\beta_0 + \\beta_1\\times mass + \\beta_2\\times VOC + \\beta_3 \\times E_{LUMO}$\n",
    "\n",
    "The extension from multiple linear regression to general additive models is straightforward; it uses the same module `LinearRegression` module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first look at the residuals, versus the function, and then the residuals versus each of the variables, and see if there is any nonlinearity. Check the RSE of this model for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = train_test_split(cld, test_size=0.95)\n",
    "features = ['mass', 'voc', 'e_lumo_alpha']\n",
    "observable = 'pce'\n",
    "train_X = train[features]\n",
    "train_Y = train[['pce']]\n",
    "test_X = test[features]\n",
    "test_Y = test[['pce']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression() \n",
    "model.fit(train_X,train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RSE =\",mean_squared_error(test_Y,model.predict(test_X)))\n",
    "plt.hexbin(test_Y,predictions-test_Y,cmap='hot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What sort of nonlinear model do these plots suggest to try?  Try something, and compare $R^2$, RSE, and plot the residuals of the new model and the new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for complicated reasons, we have to add columns to the origina dataframe , not the cleaned dataframe \n",
    "# (basically, since the cleaned file is a slice of a dataframe, not a dataframe itself)\n",
    "cld['voc2'] = cld['voc']**2  \n",
    "cld['lumo2'] = cld['e_lumo_alpha']**2\n",
    "cld['voc3'] = cld['voc']**3\n",
    "cld['lumo3'] = cld['e_lumo_alpha']**3\n",
    "cld = cld[cld['pce']!=0]  # clean out bad PCE's again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = train_test_split(cld, test_size=0.95)\n",
    "features = ['mass','voc','voc2','voc3','e_lumo_alpha','lumo2','lumo3']\n",
    "observable = 'pce'\n",
    "train_X = train[features]\n",
    "train_Y = train[['pce']]\n",
    "test_X = test[features]\n",
    "test_Y = test[['pce']]\n",
    "\n",
    "model = LinearRegression() \n",
    "model.fit(train_X,train_Y)\n",
    "\n",
    "print(\"RSE =\",mean_squared_error(test_Y,model.predict(test_X)))\n",
    "plt.hexbin(test_Y,model.predict(test_X)-test_Y,cmap='hot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Hacking time!**: What other general linear models, more complicated or simpler, can you test? How do you know if they are better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Classification using: Logistic regression\n",
    "\n",
    "Let's use a database of perovskite stability data patients to demonstrate logistic regression.  Perovskites have the general formula $ABX_3$, where $A$ and $B$ are cations and $X$ is an anion.\n",
    "\n",
    "The variables are described as follows:\n",
    "- ABX3: chemical formula of the compound\n",
    "- exp_label: is it stable?  -1 is no, 1 is yes\n",
    "- is_train: is it training data? -1 is no, 1 is yes\n",
    "- nA: $n_A$, oxiation state of A\n",
    "- nB: $n_B$, oxidation state of B\n",
    "- nX: $n_X$, oxidations state of X\n",
    "- rA (Ang):\t$r_A$, ionic radius of A in Angstroms\n",
    "- rB (Ang):\t$r_B$, ionic radius of B in Angstroms\n",
    "- rX (Ang):\t$r_X$, ionic radius of X in Angstroms\n",
    "- t: $t$, Goldschmidt tolerance factor, $\\frac{r_A + r_X}{\\sqrt{2}\\left(r_B+r_X\\right)}$\n",
    "- tau: $\\tau$, Bartel et al. tolerance factor $\\frac{r_X}{r_B}-n_A\\left(n_A - \\frac{r_A/r_B}{\\ln(r_A/r_B)}\\right)$\n",
    "- t_pred: Whether $t$ predicts it will be a perovskite, -1 is no, 1 is yes\n",
    "- tau_pred:\tWhether $\\tau$ predicts it will be a perovskite ($\\tau < 4.18$), -1 is no, 1 is yes\n",
    "- tau_prob:\tProbability that the compund it is a perovskite, according to $\\tau$\t\n",
    "\n",
    "nA,nB,nX,rA,rB,rX are the features used for training.\n",
    "\n",
    "Let's start by just looking at the means and standard deviations to see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perov=pd.read_csv('perovskite_data.csv')\n",
    "perov.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the input features which are not categorical. \n",
    "training_features = ['nA','nB','nX','rA (Ang)','rB (Ang)', 'rX (Ang)']\n",
    "table1=np.mean(perov[training_features],axis=0)\n",
    "table2=np.std(perov[training_features],axis=0)\n",
    "df = pd.DataFrame(columns=['means','stds'])\n",
    "df['means'] = table1\n",
    "df['stds'] = table2\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the other data to make sure we know what is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perov['is_train'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LogisticRegression` is in `sklearn.linear_model` and has many of the same inputs and outputs as `LinearRegression`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a `LogisticRegression` model on the six inputs, and score it. (What is the score here? is it $R^2$)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perov_training = perov[perov['is_train']==1]\n",
    "perov_testing =  perov[perov['is_train']==-1]\n",
    "inputData = perov_training[['nA','nB','nX','rA (Ang)','rB (Ang)', 'rX (Ang)']]\n",
    "outputData = (perov_training['exp_label']+1)/2  # rescale to 0,1, though it ends up not mattering.\n",
    "\n",
    "inputData_test = perov_testing[['nA','nB','nX','rA (Ang)','rB (Ang)', 'rX (Ang)']]\n",
    "outputData_test = (perov_testing['exp_label']+1)/2  # rescale to 0,1, though it ends up not mattering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit1=LogisticRegression(solver='liblinear')\n",
    "#we specify the liblinear solver to avoid a warning about not specifying\n",
    "logit1.fit(inputData,outputData)\n",
    "logit1.score(inputData,outputData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Score` for logistic regression is just the fraction that are right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(logit1.predict(inputData)==outputData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, check on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit1.score(inputData_test,outputData_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to use NO inputs, just the outputs, we would predict that the percent that are stable would be the same as the percent in our population, or about 50/50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(outputData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we are getting noticably better than random!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the documentation for the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogisticRegression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model performance\n",
    "What are the percent who are correctly classified by this model?  What are the true positive, true negative, false positive, and false negative?  Try using the `confusion_matrix` function of `sklearn.metrics`. How well does the model predict which women don't have diabetes?  How well does it predict which women do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(logit1.predict(inputData),outputData)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The format is: \n",
    "[[ True Negative  False Negative],\n",
    " [ False Positive True Positive]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at coefficients.  Can you tell which ones are most important in predicting stability?  Why or why not? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit1.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hacking Time:** How does a prediction with standardized variables do versus one without standardized ones? What do the coeficients look like before and after? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is all of the data linearly independent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are those other columns doing there?  What is `tau_pred`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix((perov_training['tau_pred']+1)/2,outputData)\n",
    "print(cm)\n",
    "cm = confusion_matrix((perov_testing['tau_pred']+1)/2,outputData_test)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean((perov_training['tau_pred']+1)/2==outputData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean((perov_testing['tau_pred']+1)/2==outputData_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
